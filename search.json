[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hey, I’m Anubhav Maity, a ML Engineer at Expedia Group. My day job involves training, deploying, and keeping an eye on ML models for fraud detection.\nOutside of my job, I immerse myself in NLP and Deep Learning through coursework, participation in Kaggle competitions, and reading of research papers, with a particular focus on Generative AI Models.\nJoin me on this blog as I share my ML experiences and insights. Let’s explore the world of AI and data together.\nThanks for stopping by!"
  },
  {
    "objectID": "posts/lsh_with_zero_for_loops/index.html",
    "href": "posts/lsh_with_zero_for_loops/index.html",
    "title": "Efficient Local Sensitive Hashing: Loop-Free Implementation",
    "section": "",
    "text": "Locality Sensitive Hashing (LSH)\nIn this blog post I have tried explaining and implementing local sensitive hashing with zero loops i.e. only tensor operations. Let’s dive in.\nLocal sensitive hashing (LSH) is a technique used in approximate nearest neighbor search and similarity-based retrieval tasks. LSH helps in efficiently finding similar items or reducing the search space for similarity queries.\nLSH works by hashing similar items into the same or nearby hash buckets with a high probability. It operates on the principle that if two items are similar, they are likely to collide (hash to the same bucket) under a certain hash function.\nLSH is “local sensitive” because it ensures that nearby items have a higher probability of being hashed into the same bucket, while items that are far apart have a lower probability of colliding. This property allows for efficient pruning of the search space, as we can focus the search on the items within the same hash buckets.\nThere are different types of LSH algorithms designed for various data types and similarity measures. Some common examples include MinHash for document similarity, SimHash for binary data, and L2-LSH for Euclidean distance-based similarity.\nLSH is particularly useful in scenarios where traditional exact search methods become impractical due to high-dimensional data or large dataset sizes. It allows for approximate similarity search with reduced computational complexity, making it a valuable tool in various applications like recommendation systems, image retrieval, and DNA sequence matching.\n\n#!pip install fastcore\n\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom fastcore.all import patch\n\n\ntorch.manual_seed(42)\ntorch.set_printoptions(precision=3, linewidth=140, sci_mode=False)\n\nI defined the class FastLSH below which takes the following arguments - dimensions: This is the number of features in the dataset. The higher the number of dimensions, the more complex the LSH algorithm will be. - hash_length: This is the length of the hash value. The longer the hash value, the more accurate the LSH algorithm will be. However, a longer hash value will also take longer to compute. - number_hash_tables: This is the number of hash tables that are used by the LSH algorithm. The more hash tables, the more likely it is that two similar items will be hashed to the same table. However, a larger number of hash tables will also take longer to search. - hash_table: This is a data structure that stores the hash values of the items in the dataset. The hash table is used to quickly find items that have similar hash values.\n\nclass FastLSH:\n    def __init__(self, dim, nht, hl):\n        self.dimensions = dim # dimensions of the data\n        self.num_hash_tables = nht # number of hash tables\n        self.hash_length = hl # hash length\n        self.hash_table = torch.randn(self.num_hash_tables, self.hash_length, self.dimensions) # the hashtable\n\n\nfastlsh = FastLSH(dim=2, nht=5, hl=10)\n\n\nfastlsh.hash_table.shape\n\ntorch.Size([5, 10, 2])\n\n\n\ndata = torch.randn(150_000, 2) # data\nquery = data[0][None] # query, adding a unit axis at the start\ndata.shape, query.shape\n\n(torch.Size([150000, 2]), torch.Size([1, 2]))\n\n\nThe purpose of the following hashing code is to apply hash functions to each data point in the input tensor. It performs a cosine similarity between the query and the hashtable, generating hash codes for every data point in the query. The resulting tensor contains these hash codes.\nThe patch decorator patches the function to the LSH class. More about patch here\n\n@patch\ndef hashing(self:FastLSH, query): \n    return (((query[:, None, None]) * self.hash_table).sum(-1) &gt;= 0).long()\n\nLet’s utilize the hashing function mentioned above to hash both the query and data points.\n\ndata_hash = fastlsh.hashing(data)\ndata_hash.shape\n\ntorch.Size([150000, 5, 10])\n\n\nBased on the obtained data_hash.shape from the above, we can observe that each of the 150_000 data points has been hashed using 5 hash functions, resulting in hash codes of length 10 for each data point.\n\nquery_hash = fastlsh.hashing(query)\nquery_hash.shape\n\ntorch.Size([1, 5, 10])\n\n\nNow, let’s proceed to obtain the indexes of data where the hash code of each data point matches the hash code of the query point.\nTo determine if the hash codes are the same, we can begin by calculating the dot product along the last axis and dividing it by the sum of the corresponding axis in the data. If the resulting values are equal to 1, it indicates that the hash codes are the same. Otherwise, if the values differ from 1, it implies that the hash codes are not the same.\n\n(query_hash * data_hash).sum(-1)\n\ntensor([[4, 5, 2, 6, 5],\n        [1, 3, 2, 2, 3],\n        [4, 3, 0, 4, 3],\n        ...,\n        [2, 3, 2, 5, 4],\n        [1, 3, 2, 3, 3],\n        [0, 1, 2, 2, 1]])\n\n\n\ndata_hash.sum(-1)\n\ntensor([[ 4,  5,  2,  6,  5],\n        [ 4,  8,  9,  6,  6],\n        [ 6,  3,  0,  4,  5],\n        ...,\n        [ 5,  6,  4,  8,  4],\n        [ 4,  7,  7,  7,  6],\n        [ 4,  6, 10,  6,  4]])\n\n\n\n(query_hash * data_hash).shape\n\ntorch.Size([150000, 5, 10])\n\n\n\n(query_hash * data_hash).sum(-1) / data_hash.sum(-1) \n\ntensor([[1.000, 1.000, 1.000, 1.000, 1.000],\n        [0.250, 0.375, 0.222, 0.333, 0.500],\n        [0.667, 1.000,   nan, 1.000, 0.600],\n        ...,\n        [0.400, 0.500, 0.500, 0.625, 1.000],\n        [0.250, 0.429, 0.286, 0.429, 0.500],\n        [0.000, 0.167, 0.200, 0.333, 0.250]])\n\n\n\nresult = ( (query_hash * data_hash).sum(-1) / data_hash.sum(-1) ) == 1\nresult\n\ntensor([[ True,  True,  True,  True,  True],\n        [False, False, False, False, False],\n        [False,  True, False,  True, False],\n        ...,\n        [False, False, False, False,  True],\n        [False, False, False, False, False],\n        [False, False, False, False, False]])\n\n\nWe can obtain the indices where the values are True using the following code.\n\nresult_indices = torch.nonzero(torch.any(result, dim=1)).flatten()\nresult_indices.shape\n\ntorch.Size([50337])\n\n\n\ndata[result_indices].shape\n\ntorch.Size([50337, 2])\n\n\nNow that we have obtained the indices, let’s proceed to compute the Euclidean distance (L2 norm).\nTo compute the Euclidean distance, we don’t need to calculate the distance from every one of the 150_000 points. Instead, we only need to compute the distance from the points where the hash codes are the same, as we have already determined from the indices obtained.\n\n((query - data[result_indices])**2).sum(-1).sqrt().shape\n\ntorch.Size([50337])\n\n\nWe can consolidate all of the aforementioned operations into a single function by defining the following function\n\n@patch\ndef query_neigbours(self:FastLSH, query, data, data_hash, neighbours=10):\n    query_hash = self.hashing(query)\n    result = ( (query_hash * data_hash).sum(-1) / data_hash.sum(-1) ) == 1\n    result_indices = torch.nonzero(torch.any(result, dim=1)).flatten()\n    \n    dist = ((query - data[result_indices]) ** 2).sum(-1).sqrt()\n    sorted_dist, idx = torch.sort(dist)\n    \n    \n    return sorted_dist[:neighbours], result_indices[idx[:neighbours]]\n\n\nquery = data[0][None]; query.shape\n\ntorch.Size([1, 2])\n\n\n\ndata_hash = fastlsh.hashing(data)\nfastlsh.query_neigbours(query, data, data_hash, 10)\n\n(tensor([0.000, 0.004, 0.004, 0.005, 0.007, 0.007, 0.009, 0.009, 0.009, 0.010]),\n tensor([     0,  16866,  13708,  29511,  37183,  31814,   9378, 122251, 131806,   4483]))\n\n\n\n%timeit -n 5 _=fastlsh.query_neigbours(query, data, data_hash, 10)\n\n11.9 ms ± 643 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\n\nGPU\nI’m currently unable to determine how to pass batches of query points for computing the hash and obtaining the distances. As a temporary solution, I’m passing a single query point and the data as CUDA. Let’s see if this optimization yields better results compared to the CPU version mentioned above.\n\ndata_cuda = data.cuda()\nquery_cuda = query.cuda()\nfastlsh.hash_table = fastlsh.hash_table.cuda()\n\n\ndata_hash_cuda = fastlsh.hashing(data_cuda)\n\n\nfastlsh.query_neigbours(query_cuda, data_cuda, data_hash_cuda, 10)\n\n(tensor([0.000, 0.004, 0.004, 0.005, 0.007, 0.007, 0.009, 0.009, 0.009, 0.010], device='cuda:0'),\n tensor([     0,  16866,  13708,  29511,  37183,  31814,   9378, 122251, 131806,   4483], device='cuda:0'))\n\n\n\n%timeit -n 5 _=fastlsh.query_neigbours(query_cuda, data_cuda, data_hash_cuda, 10)\n\n739 µs ± 30.9 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n\n\nThe GPU computation is 15x faster"
  },
  {
    "objectID": "posts/movie_genre_prediction_using_hf_transformer/index.html",
    "href": "posts/movie_genre_prediction_using_hf_transformer/index.html",
    "title": "Movie Genre Predictions with Hugging Face Transformers",
    "section": "",
    "text": "I just finished Part 1 of Hugging Face NLP course and wanted to put my new skills to the test. I stumbled upon a Hugging Face competition called Movie Genre Prediction, where the challenge is to guess the movie genre based on the synopsis and title. In this blog post, I’ll share my journey and findings. Let’s dive in!\nLets install the following packages by uncommenting the following code if not installed already\n\n# !pip install datasets\n# !pip install transformers -U\n# !pip install huggingface_hub\n# !pip install rich\n# !pip install accelerate -U\n# !pip install evaluate\n\nFollowing are the steps to create hugging face credentials token which be needed when using notebook_login below\n\nCreate a Hugging Face account (if you don’t have one): If you don’t already have an account on the Hugging Face website, you’ll need to create one. Visit the Hugging Face website (https://huggingface.co/) and sign up for an account.\nLog in to your Hugging Face account: Use your credentials to log in to your Hugging Face account.\nGenerate an API token: Hugging Face provides API tokens for authentication. To generate an API token, go to your account settings on the Hugging Face website. You can usually find this in your account dashboard or profile settings.\nGenerate the token: Once you’re in your account settings, look for an option related to API tokens or credentials. You should find an option to generate a new token. Click on it, and the system will generate a unique API token for you.\nCopy the API token: Once the token is generated, you’ll typically see it displayed on the screen. It might be a long string of characters. Copy this token to your clipboard.\nStore the token securely: API tokens are sensitive credentials, so it’s essential to store them securely. You should never share your API token publicly or expose it in your code repositories.\n\nNow, you have your Hugging Face API token, which you can use for authentication when making requests to the Hugging Face API or accessing resources on the Hugging Face Model Hub.\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n\n\n\nLets import the following pacakges\n\nfrom transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\nfrom datasets import load_dataset, Dataset\nfrom collections import Counter\nimport evaluate\nimport numpy as np\nfrom rich import print\nimport pandas as pd\n\n\n\nWe will be using the datadrivenscience/movie-genre-prediction competition dataset for model training. You can read more about the competition here and the dataset here.\n\ndataset = load_dataset(\"datadrivenscience/movie-genre-prediction\"); dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 54000\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 36000\n    })\n})\n\n\nThe dataset has train and test splits with following features - id - movie name - synopsis - genre\n\nprint(dataset['train'][:3])\n\n{\n    'id': [44978, 50185, 34131],\n    'movie_name': ['Super Me', 'Entity Project', 'Behavioral Family Therapy for Serious Psychiatric Disorders'],\n    'synopsis': [\n        'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a \ndemon. Selling them makes him rich.',\n        'A director and her friends renting a haunted house to capture paranormal events in order to prove it and \nbecome popular.',\n        'This is an educational video for families and family therapists that describes the Behavioral Family \nTherapy approach to dealing with serious psychiatric illnesses.'\n    ],\n    'genre': ['fantasy', 'horror', 'family']\n}\n\n\n\nAbove we have sliced and printed 3 rows of training dataset\n\nlabels = set(dataset['train']['genre'])\nnum_labels = len(labels)\nlabels\n\n{'action',\n 'adventure',\n 'crime',\n 'family',\n 'fantasy',\n 'horror',\n 'mystery',\n 'romance',\n 'scifi',\n 'thriller'}\n\n\nThere are 10 genres, - action - adventure - crime - family - fantasy - horror - mystery - romance - scifi - thriller\n\nlabels_count = Counter(dataset['train']['genre']); print(labels_count)\n\nCounter({\n    'fantasy': 5400,\n    'horror': 5400,\n    'family': 5400,\n    'scifi': 5400,\n    'action': 5400,\n    'crime': 5400,\n    'adventure': 5400,\n    'mystery': 5400,\n    'romance': 5400,\n    'thriller': 5400\n})\n\n\n\nLooks like the labels are evenly sampled, everyone has count of 5400. Thats good.\n\ndataset = dataset.rename_column('genre', 'labels')\n\n\ndataset = dataset.class_encode_column(\"labels\")\n\n\n\n\n\n\n\nIn the above steps we rename the column genre to labels to mark the genre as target variable\nThen we convert the labels to ClassLabel type\n\ndataset['train'].features['labels']\n\nClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None)\n\n\nConverting labels to the ClassLabel type in the Hugging Face library helps with:\n\nConsistency: It makes your labels work smoothly with the library’s tools and models.\nNumber Conversion: It turns text labels into numbers, which some models need.\nEasy Mapping: It simplifies translating between text labels and numbers.\n\n\n\n\nI could not find a bettery way to remove duplicates than converting to pandas, removing duplicates and converting back to hugging face dataset.\n\ntrain_df = pd.DataFrame(dataset['train']) # converting to pandas\n\n\ntrain_df = train_df.drop_duplicates(['movie_name', 'synopsis']) # Removes duplicates based on `movie_name` and `synopsis` attributes\n\n\nds = Dataset.from_pandas(train_df) # converting back to dataset\n\n\nds.features['labels']\n\nValue(dtype='int64', id=None)\n\n\nWhen converting a dataset from pandas, it creates the label type as Value. However, we will need to subsequently convert it to ClassLabel.\n\nds = ds.class_encode_column(\"labels\"); ds # \n\n\n\n\n\n\n\nDataset({\n    features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__'],\n    num_rows: 46344\n})\n\n\n\n\n\n\ncheckpoint = \"bert-base-uncased\"\n\nA checkpoint is a saved model state, including its architecture and trained weights, which can be used for various NLP tasks and fine-tuning.\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer('Movie Genre Predictions with Hugging Face Transformers')\n\n{'input_ids': [101, 3185, 6907, 20932, 2007, 17662, 2227, 19081, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nAbove we load the tokenizer and use it on a sentence. Loading a checkpoint of a tokenizer associated with a pretrained language model is necessary to maintain consistency in the tokenization process. This ensures that your input text is processed in a way that aligns with the model’s pre-existing knowledge and allows you to use the pretrained model effectively\nWhat is attention_mask? &gt; Sometimes, we want to tell the computer which parts of the sentence are important and which are not. The attention mask is like a spotlight. It’s a list of 1s and 0s, where 1 means “pay attention” and 0 means “ignore.” For our sentence, it could be [1, 1, 1, 1, 1] because we want the computer to pay attention to all tokens.\nWhat is token_type_ids? &gt; If you have multiple sentences, you’d want the computer to know which sentence each token belongs to. Token Type IDs help with that. For one sentence, it’s all 0s. If you had two sentences, the first sentence would have 0s, and the second sentence would have 1s.\nLet’s break down the process of creating input_ids below into following steps:\n\n\nImagine you have a sentence, “Hugging Face is awesome!” To help a computer understand it, you first split it into smaller parts, like words: [“Hugging”, “Face”, “is”, “awesome”, “!”]. These smaller parts are called tokens.\nWe can tokenize the synopsis of the first row of training set\n\ndataset['train'][0]['synopsis']\n\n'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.'\n\n\n\ntokens = tokenizer.tokenize(dataset['train'][0]['synopsis']); tokens\n\n['a',\n 'young',\n 'script',\n '##writer',\n 'starts',\n 'bringing',\n 'valuable',\n 'objects',\n 'back',\n 'from',\n 'his',\n 'short',\n 'nightmares',\n 'of',\n 'being',\n 'chased',\n 'by',\n 'a',\n 'demon',\n '.',\n 'selling',\n 'them',\n 'makes',\n 'him',\n 'rich',\n '.']\n\n\n\n\n\nComputers prefer numbers, so we need to convert these tokens into unique numbers. Each token gets a special ID. For example, “Hugging” might be ID 101, “Face” might be ID 102, and so on. The sentence becomes a list of IDs: [101, 102, 103, 104, 105].\n\nids = tokenizer.convert_tokens_to_ids(tokens); ids\n\n[1037,\n 2402,\n 5896,\n 15994,\n 4627,\n 5026,\n 7070,\n 5200,\n 2067,\n 2013,\n 2010,\n 2460,\n 15446,\n 1997,\n 2108,\n 13303,\n 2011,\n 1037,\n 5698,\n 1012,\n 4855,\n 2068,\n 3084,\n 2032,\n 4138,\n 1012]\n\n\nIn summary, Hugging Face tokenization takes your text, breaks it into tokens (smaller parts), gives each token a unique ID, creates an attention mask to say what’s important, and token type IDs to track different sentences if needed.\n\nds = ds.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n\nThe above code splits a dataset (ds) into two parts: a training set and a testing set.\n\nThe test_size=0.2 means that 20% of the data will be used for testing, and the remaining 80% for training.\nstratify_by_column=\"labels\" means that the split will ensure both the training and testing sets have a similar distribution of labels (class proportions) as in the original dataset. This is useful for maintaining balance in classification tasks.\n\n\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__'],\n        num_rows: 37075\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__'],\n        num_rows: 9269\n    })\n})\n\n\n\ndef tokenize(sample):\n    sample['text'] = list(map(lambda x: ': '.join(x), zip(sample['movie_name'], sample['synopsis']))) \n    return tokenizer(sample['text'], truncation=True)\n\n\ntokenized_ds = ds.map(tokenize, batched=True); tokenized_ds\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 37075\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9269\n    })\n})\n\n\nThe above code defines a tokenize function that combines text from two columns, movie_name and synopsis, and then tokenizes the combined text using a tokenizer.\n\nsample[text] is created by joining movie_name and synopsis with a colon and space.\nThe tokenizer is applied to the text column with truncation enabled.\n\nAfter defining the function, it’s applied to a dataset (ds) using .map() to tokenize the text in batches, and the result is stored in tokenized_ds.\n\n\n\n\n\ntraining_args = TrainingArguments('movie-genre-predictions', \n                                  evaluation_strategy = 'epoch',\n                                  per_device_train_batch_size = 32,\n                                  per_device_eval_batch_size = 64,\n                                  save_strategy = 'epoch',\n                                  push_to_hub = True, \n                                  learning_rate = 1e-5\n                                 )\n\nThe above code sets up the configuration for training a Hugging Face model, for a movie genre prediction task. Let’s break it down step by step:\n\nTrainingArguments: This is a special object or data structure that holds various settings and options for training a machine learning model.\n'movie-genre-predictions': It’s naming the training process or giving it a unique name. It’s like giving a name to a file so you can easily identify it later.\nevaluation_strategy = 'epoch': This line specifies how often the model’s performance should be evaluated. In this case, it’s set to epoch, which means after every complete pass through the training data. An epoch is like a full round of training.\nper_device_train_batch_size = 32: This indicates how many examples or data points should be processed at once on each processing unit during training. It’s set to 32, so 32 data points will be processed together in parallel.\nper_device_eval_batch_size = 64: Similar to the previous line, but this one specifies the batch size for evaluation (measuring how well the model is doing). It’s set to 64, so 64 examples will be evaluated at once.\nsave_strategy = 'epoch': This determines when the model’s checkpoints (saves of the model’s progress) should be saved. Again, it’s set to epoch, meaning after each training round.\npush_to_hub = True: This is likely specific to the Hugging Face Transformers library. If set to True, it means that the model checkpoints will be pushed or uploaded to the Hugging Face Model Hub, a place to store and share models.\nlearning_rate: This is like a step size or a pace setter for a machine learning model when it’s trying to learn from data. Imagine you’re trying to find the lowest point in a hilly area by taking small steps downhill. The learning rate determines how big or small those steps should be.\n\nIn simple terms, this code is configuring how a machine learning model should be trained for movie genre prediction. It sets up details like when to check how well the model is doing, how much data to process at a time, and where to save the model’s progress. It also says that the model checkpoints should be uploaded to the Hugging Face Model Hub.\nYou may see more details about TrainingArguments here\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = num_labels)\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nAbove we load the model for Sequence Classification of 10 labels\n\nclf_metrics = evaluate.load(\"accuracy\")\n\nThe evaluate library provides the metrics on which to evaluate the validation set. Above I have choosen accuracy as the metrics (The accuracy metric is used in the competition too)\n\ndef compute_metrics(batch):\n    logits, labels = batch\n    predictions = np.argmax(logits, axis=-1) # finds the position (index) of the highest value in a NumPy array and returns that index as an integer.\n    return clf_metrics.compute(predictions=predictions, references=labels)\n\nI have defined compute_metrics to compute the metrics after each epoch on validation set\n\ntrainer = Trainer(model, \n                  args = training_args,\n                  train_dataset = tokenized_ds['train'],\n                  eval_dataset = tokenized_ds['test'], \n                  tokenizer = tokenizer,\n                  compute_metrics = compute_metrics\n                 )\n\nThe Trainer function in Hugging Face simplifies the process of fine-tuning pre-trained NLP models for specific tasks. It handles data loading, training, evaluation, and model saving, making it easier to customize and use these models for various NLP tasks.\n\ntrainer.train() # the model is training\n\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [3477/3477 09:27, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n1.664900\n1.585141\n0.446542\n\n\n2\n1.522100\n1.557735\n0.456576\n\n\n3\n1.422900\n1.559032\n0.458733\n\n\n\n\n\n\nTrainOutput(global_step=3477, training_loss=1.5756116926858643, metrics={'train_runtime': 568.5141, 'train_samples_per_second': 195.642, 'train_steps_per_second': 6.116, 'total_flos': 3706787213357172.0, 'train_loss': 1.5756116926858643, 'epoch': 3.0})\n\n\n\ntrainer.push_to_hub()\n\n'https://huggingface.co/anubhavmaity/movie-genre-predictions/tree/main/'\n\n\nThe above code pushes the model to model hub and creates the model card. The model card is here\n\n\n\n\ntokenized_test_ds = dataset[\"test\"].map(tokenize, batched=True)\n\nWe apply the same tokenization method to the test dataset as we did for the training dataset above.\n\ntest_logits = trainer.predict(tokenized_test_ds)\n\n\n\n\n\ntest_logits.predictions.shape\n\n(36000, 10)\n\n\nThe predict function on tokenized_dataset throws out logits\n\ntest_predictions = np.argmax(test_logits.predictions, axis=-1)\n\nWe get the index with the highest value along the last dimension\n\npredicted_genre = dataset[\"train\"].features[\"labels\"].int2str(test_predictions)\n\nHere we convert the index to corresponding genre\n\ndf = pd.DataFrame({'id':tokenized_test_ds['id'],\n                  'genre': predicted_genre})   # creating dataframe with `id` and `genre` as columns for submission\n\n\ndf.to_csv('submission.csv')\n\nSubmitting the submission.csv got me the following score - Public Score: 0.4176 - Private Score: 0.4162\nwhich ranks me around 27th rank. The 1st rank has accuracy of 0.4456 and 0.4412 in the public and private leaderboard respectively\nWe can improve the score by using following strategies 1. Training with a bigger architecture 2. Training for more epochs 3. Ensembling with other models"
  },
  {
    "objectID": "posts/movie_genre_prediction_using_hf_transformer/index.html#datasets",
    "href": "posts/movie_genre_prediction_using_hf_transformer/index.html#datasets",
    "title": "Movie Genre Predictions with Hugging Face Transformers",
    "section": "",
    "text": "We will be using the datadrivenscience/movie-genre-prediction competition dataset for model training. You can read more about the competition here and the dataset here.\n\ndataset = load_dataset(\"datadrivenscience/movie-genre-prediction\"); dataset\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 54000\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'genre'],\n        num_rows: 36000\n    })\n})\n\n\nThe dataset has train and test splits with following features - id - movie name - synopsis - genre\n\nprint(dataset['train'][:3])\n\n{\n    'id': [44978, 50185, 34131],\n    'movie_name': ['Super Me', 'Entity Project', 'Behavioral Family Therapy for Serious Psychiatric Disorders'],\n    'synopsis': [\n        'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a \ndemon. Selling them makes him rich.',\n        'A director and her friends renting a haunted house to capture paranormal events in order to prove it and \nbecome popular.',\n        'This is an educational video for families and family therapists that describes the Behavioral Family \nTherapy approach to dealing with serious psychiatric illnesses.'\n    ],\n    'genre': ['fantasy', 'horror', 'family']\n}\n\n\n\nAbove we have sliced and printed 3 rows of training dataset\n\nlabels = set(dataset['train']['genre'])\nnum_labels = len(labels)\nlabels\n\n{'action',\n 'adventure',\n 'crime',\n 'family',\n 'fantasy',\n 'horror',\n 'mystery',\n 'romance',\n 'scifi',\n 'thriller'}\n\n\nThere are 10 genres, - action - adventure - crime - family - fantasy - horror - mystery - romance - scifi - thriller\n\nlabels_count = Counter(dataset['train']['genre']); print(labels_count)\n\nCounter({\n    'fantasy': 5400,\n    'horror': 5400,\n    'family': 5400,\n    'scifi': 5400,\n    'action': 5400,\n    'crime': 5400,\n    'adventure': 5400,\n    'mystery': 5400,\n    'romance': 5400,\n    'thriller': 5400\n})\n\n\n\nLooks like the labels are evenly sampled, everyone has count of 5400. Thats good.\n\ndataset = dataset.rename_column('genre', 'labels')\n\n\ndataset = dataset.class_encode_column(\"labels\")\n\n\n\n\n\n\n\nIn the above steps we rename the column genre to labels to mark the genre as target variable\nThen we convert the labels to ClassLabel type\n\ndataset['train'].features['labels']\n\nClassLabel(names=['action', 'adventure', 'crime', 'family', 'fantasy', 'horror', 'mystery', 'romance', 'scifi', 'thriller'], id=None)\n\n\nConverting labels to the ClassLabel type in the Hugging Face library helps with:\n\nConsistency: It makes your labels work smoothly with the library’s tools and models.\nNumber Conversion: It turns text labels into numbers, which some models need.\nEasy Mapping: It simplifies translating between text labels and numbers."
  },
  {
    "objectID": "posts/movie_genre_prediction_using_hf_transformer/index.html#remove-duplicates",
    "href": "posts/movie_genre_prediction_using_hf_transformer/index.html#remove-duplicates",
    "title": "Movie Genre Predictions with Hugging Face Transformers",
    "section": "",
    "text": "I could not find a bettery way to remove duplicates than converting to pandas, removing duplicates and converting back to hugging face dataset.\n\ntrain_df = pd.DataFrame(dataset['train']) # converting to pandas\n\n\ntrain_df = train_df.drop_duplicates(['movie_name', 'synopsis']) # Removes duplicates based on `movie_name` and `synopsis` attributes\n\n\nds = Dataset.from_pandas(train_df) # converting back to dataset\n\n\nds.features['labels']\n\nValue(dtype='int64', id=None)\n\n\nWhen converting a dataset from pandas, it creates the label type as Value. However, we will need to subsequently convert it to ClassLabel.\n\nds = ds.class_encode_column(\"labels\"); ds # \n\n\n\n\n\n\n\nDataset({\n    features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__'],\n    num_rows: 46344\n})"
  },
  {
    "objectID": "posts/movie_genre_prediction_using_hf_transformer/index.html#tokenization",
    "href": "posts/movie_genre_prediction_using_hf_transformer/index.html#tokenization",
    "title": "Movie Genre Predictions with Hugging Face Transformers",
    "section": "",
    "text": "checkpoint = \"bert-base-uncased\"\n\nA checkpoint is a saved model state, including its architecture and trained weights, which can be used for various NLP tasks and fine-tuning.\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenizer('Movie Genre Predictions with Hugging Face Transformers')\n\n{'input_ids': [101, 3185, 6907, 20932, 2007, 17662, 2227, 19081, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nAbove we load the tokenizer and use it on a sentence. Loading a checkpoint of a tokenizer associated with a pretrained language model is necessary to maintain consistency in the tokenization process. This ensures that your input text is processed in a way that aligns with the model’s pre-existing knowledge and allows you to use the pretrained model effectively\nWhat is attention_mask? &gt; Sometimes, we want to tell the computer which parts of the sentence are important and which are not. The attention mask is like a spotlight. It’s a list of 1s and 0s, where 1 means “pay attention” and 0 means “ignore.” For our sentence, it could be [1, 1, 1, 1, 1] because we want the computer to pay attention to all tokens.\nWhat is token_type_ids? &gt; If you have multiple sentences, you’d want the computer to know which sentence each token belongs to. Token Type IDs help with that. For one sentence, it’s all 0s. If you had two sentences, the first sentence would have 0s, and the second sentence would have 1s.\nLet’s break down the process of creating input_ids below into following steps:\n\n\nImagine you have a sentence, “Hugging Face is awesome!” To help a computer understand it, you first split it into smaller parts, like words: [“Hugging”, “Face”, “is”, “awesome”, “!”]. These smaller parts are called tokens.\nWe can tokenize the synopsis of the first row of training set\n\ndataset['train'][0]['synopsis']\n\n'A young scriptwriter starts bringing valuable objects back from his short nightmares of being chased by a demon. Selling them makes him rich.'\n\n\n\ntokens = tokenizer.tokenize(dataset['train'][0]['synopsis']); tokens\n\n['a',\n 'young',\n 'script',\n '##writer',\n 'starts',\n 'bringing',\n 'valuable',\n 'objects',\n 'back',\n 'from',\n 'his',\n 'short',\n 'nightmares',\n 'of',\n 'being',\n 'chased',\n 'by',\n 'a',\n 'demon',\n '.',\n 'selling',\n 'them',\n 'makes',\n 'him',\n 'rich',\n '.']\n\n\n\n\n\nComputers prefer numbers, so we need to convert these tokens into unique numbers. Each token gets a special ID. For example, “Hugging” might be ID 101, “Face” might be ID 102, and so on. The sentence becomes a list of IDs: [101, 102, 103, 104, 105].\n\nids = tokenizer.convert_tokens_to_ids(tokens); ids\n\n[1037,\n 2402,\n 5896,\n 15994,\n 4627,\n 5026,\n 7070,\n 5200,\n 2067,\n 2013,\n 2010,\n 2460,\n 15446,\n 1997,\n 2108,\n 13303,\n 2011,\n 1037,\n 5698,\n 1012,\n 4855,\n 2068,\n 3084,\n 2032,\n 4138,\n 1012]\n\n\nIn summary, Hugging Face tokenization takes your text, breaks it into tokens (smaller parts), gives each token a unique ID, creates an attention mask to say what’s important, and token type IDs to track different sentences if needed.\n\nds = ds.train_test_split(test_size=0.2, stratify_by_column=\"labels\")\n\nThe above code splits a dataset (ds) into two parts: a training set and a testing set.\n\nThe test_size=0.2 means that 20% of the data will be used for testing, and the remaining 80% for training.\nstratify_by_column=\"labels\" means that the split will ensure both the training and testing sets have a similar distribution of labels (class proportions) as in the original dataset. This is useful for maintaining balance in classification tasks.\n\n\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__'],\n        num_rows: 37075\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__'],\n        num_rows: 9269\n    })\n})\n\n\n\ndef tokenize(sample):\n    sample['text'] = list(map(lambda x: ': '.join(x), zip(sample['movie_name'], sample['synopsis']))) \n    return tokenizer(sample['text'], truncation=True)\n\n\ntokenized_ds = ds.map(tokenize, batched=True); tokenized_ds\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 37075\n    })\n    test: Dataset({\n        features: ['id', 'movie_name', 'synopsis', 'labels', '__index_level_0__', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9269\n    })\n})\n\n\nThe above code defines a tokenize function that combines text from two columns, movie_name and synopsis, and then tokenizes the combined text using a tokenizer.\n\nsample[text] is created by joining movie_name and synopsis with a colon and space.\nThe tokenizer is applied to the text column with truncation enabled.\n\nAfter defining the function, it’s applied to a dataset (ds) using .map() to tokenize the text in batches, and the result is stored in tokenized_ds."
  },
  {
    "objectID": "posts/movie_genre_prediction_using_hf_transformer/index.html#training",
    "href": "posts/movie_genre_prediction_using_hf_transformer/index.html#training",
    "title": "Movie Genre Predictions with Hugging Face Transformers",
    "section": "",
    "text": "training_args = TrainingArguments('movie-genre-predictions', \n                                  evaluation_strategy = 'epoch',\n                                  per_device_train_batch_size = 32,\n                                  per_device_eval_batch_size = 64,\n                                  save_strategy = 'epoch',\n                                  push_to_hub = True, \n                                  learning_rate = 1e-5\n                                 )\n\nThe above code sets up the configuration for training a Hugging Face model, for a movie genre prediction task. Let’s break it down step by step:\n\nTrainingArguments: This is a special object or data structure that holds various settings and options for training a machine learning model.\n'movie-genre-predictions': It’s naming the training process or giving it a unique name. It’s like giving a name to a file so you can easily identify it later.\nevaluation_strategy = 'epoch': This line specifies how often the model’s performance should be evaluated. In this case, it’s set to epoch, which means after every complete pass through the training data. An epoch is like a full round of training.\nper_device_train_batch_size = 32: This indicates how many examples or data points should be processed at once on each processing unit during training. It’s set to 32, so 32 data points will be processed together in parallel.\nper_device_eval_batch_size = 64: Similar to the previous line, but this one specifies the batch size for evaluation (measuring how well the model is doing). It’s set to 64, so 64 examples will be evaluated at once.\nsave_strategy = 'epoch': This determines when the model’s checkpoints (saves of the model’s progress) should be saved. Again, it’s set to epoch, meaning after each training round.\npush_to_hub = True: This is likely specific to the Hugging Face Transformers library. If set to True, it means that the model checkpoints will be pushed or uploaded to the Hugging Face Model Hub, a place to store and share models.\nlearning_rate: This is like a step size or a pace setter for a machine learning model when it’s trying to learn from data. Imagine you’re trying to find the lowest point in a hilly area by taking small steps downhill. The learning rate determines how big or small those steps should be.\n\nIn simple terms, this code is configuring how a machine learning model should be trained for movie genre prediction. It sets up details like when to check how well the model is doing, how much data to process at a time, and where to save the model’s progress. It also says that the model checkpoints should be uploaded to the Hugging Face Model Hub.\nYou may see more details about TrainingArguments here\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels = num_labels)\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nAbove we load the model for Sequence Classification of 10 labels\n\nclf_metrics = evaluate.load(\"accuracy\")\n\nThe evaluate library provides the metrics on which to evaluate the validation set. Above I have choosen accuracy as the metrics (The accuracy metric is used in the competition too)\n\ndef compute_metrics(batch):\n    logits, labels = batch\n    predictions = np.argmax(logits, axis=-1) # finds the position (index) of the highest value in a NumPy array and returns that index as an integer.\n    return clf_metrics.compute(predictions=predictions, references=labels)\n\nI have defined compute_metrics to compute the metrics after each epoch on validation set\n\ntrainer = Trainer(model, \n                  args = training_args,\n                  train_dataset = tokenized_ds['train'],\n                  eval_dataset = tokenized_ds['test'], \n                  tokenizer = tokenizer,\n                  compute_metrics = compute_metrics\n                 )\n\nThe Trainer function in Hugging Face simplifies the process of fine-tuning pre-trained NLP models for specific tasks. It handles data loading, training, evaluation, and model saving, making it easier to customize and use these models for various NLP tasks.\n\ntrainer.train() # the model is training\n\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n\n    \n      \n      \n      [3477/3477 09:27, Epoch 3/3]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nAccuracy\n\n\n\n\n1\n1.664900\n1.585141\n0.446542\n\n\n2\n1.522100\n1.557735\n0.456576\n\n\n3\n1.422900\n1.559032\n0.458733\n\n\n\n\n\n\nTrainOutput(global_step=3477, training_loss=1.5756116926858643, metrics={'train_runtime': 568.5141, 'train_samples_per_second': 195.642, 'train_steps_per_second': 6.116, 'total_flos': 3706787213357172.0, 'train_loss': 1.5756116926858643, 'epoch': 3.0})\n\n\n\ntrainer.push_to_hub()\n\n'https://huggingface.co/anubhavmaity/movie-genre-predictions/tree/main/'\n\n\nThe above code pushes the model to model hub and creates the model card. The model card is here"
  },
  {
    "objectID": "posts/movie_genre_prediction_using_hf_transformer/index.html#submitting-to-the-competition",
    "href": "posts/movie_genre_prediction_using_hf_transformer/index.html#submitting-to-the-competition",
    "title": "Movie Genre Predictions with Hugging Face Transformers",
    "section": "",
    "text": "tokenized_test_ds = dataset[\"test\"].map(tokenize, batched=True)\n\nWe apply the same tokenization method to the test dataset as we did for the training dataset above.\n\ntest_logits = trainer.predict(tokenized_test_ds)\n\n\n\n\n\ntest_logits.predictions.shape\n\n(36000, 10)\n\n\nThe predict function on tokenized_dataset throws out logits\n\ntest_predictions = np.argmax(test_logits.predictions, axis=-1)\n\nWe get the index with the highest value along the last dimension\n\npredicted_genre = dataset[\"train\"].features[\"labels\"].int2str(test_predictions)\n\nHere we convert the index to corresponding genre\n\ndf = pd.DataFrame({'id':tokenized_test_ds['id'],\n                  'genre': predicted_genre})   # creating dataframe with `id` and `genre` as columns for submission\n\n\ndf.to_csv('submission.csv')\n\nSubmitting the submission.csv got me the following score - Public Score: 0.4176 - Private Score: 0.4162\nwhich ranks me around 27th rank. The 1st rank has accuracy of 0.4456 and 0.4412 in the public and private leaderboard respectively\nWe can improve the score by using following strategies 1. Training with a bigger architecture 2. Training for more epochs 3. Ensembling with other models"
  },
  {
    "objectID": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html",
    "href": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html",
    "title": "Paper Implementation: A Neural Algorithm of Artistic Style",
    "section": "",
    "text": "Here we will implement the paper Neural Algorithm of Artistic Style by Gatys et. al. I came across this paper and the concept in the Johno Whitaker’s lecture on Style Transfer. The video is a must watch. We will use the minai library. The minai library is implemented in the Part 2 of the fastai course. The library is built over pytorch and easy to use.\n\nThe neural algorithm of artistic style is a deep learning technique developed to transfer the style of one image (typically an artwork) onto another image (typically a photograph), creating a new image that combines the content of the photograph with the style of the artwork. Key Concepts:\n1. Convolutional Neural Networks (CNNs): The algorithm uses a pre-trained CNN, such as VGG-19, to extract features from images at multiple layers. Lower layers capture detailed textures and edges, while higher layers capture more abstract content and style information.\n\n2. Content Representation: The content of an image is represented by the activations of higher layers of the CNN. These layers encode the spatial arrangement of objects in the image.\n\n3. Style Representation: The style of an image is represented using Gram matrices of the activations from multiple layers of the CNN. The Gram matrix captures the correlations between different filter responses, representing textures and patterns characteristic of the style.\n\n4. Optimization Process: The style transfer is achieved by iteratively updating a white noise image (or the content image) to minimize two losses:\n    - Content Loss: Ensures that the generated image retains the content of the original photograph. This is typically the squared error between the feature representations of the content image and the generated image at a particular layer.\n    - Style Loss: Ensures that the generated image captures the style of the artwork. This is calculated as the squared error between the Gram matrices of the style image and the generated image at multiple layers.\nProcess:\n1. Extract content features from a specific layer of the CNN for the content image.\n2. Extract style features (Gram matrices) from multiple layers of the CNN for the style image.\n3. Initialize a generated image (either white noise or the content image).\n4. Define a loss function combining content loss and style loss.\n5. Use gradient descent to iteratively update the generated image to minimize the loss function.\n6. The final result is an image that combines the content of the photograph with the artistic style of the artwork.\nThis algorithm has been widely used and extended in various applications, including stylizing videos, real-time style transfer, and more. It represents a significant advancement in the field of neural networks and computer vision.\n\nimport timm\nimport fastcore.all as fc\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torchvision\n\nfrom minai.datasets import show_image, show_images\nfrom minai.learner import TrainCB, DeviceCB, ProgressCB, MetricsCB, Learner, DataLoaders, Callback, def_device, to_cpu\n\n/home/conda-envs/fastai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nIn the paper the authors have used vgg19 as the pretrained model, we can download that using timm’s library\n\ntimm.list_models('*vgg19*', pretrained=True)\n\n['vgg19.tv_in1k', 'vgg19_bn.tv_in1k']\n\n\n\nvgg19 = timm.create_model('vgg19', pretrained=True).to(def_device).features\n\n\nvgg19\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace=True)\n  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace=True)\n  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace=True)\n  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace=True)\n  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace=True)\n  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (17): ReLU(inplace=True)\n  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace=True)\n  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace=True)\n  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (24): ReLU(inplace=True)\n  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (26): ReLU(inplace=True)\n  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace=True)\n  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (31): ReLU(inplace=True)\n  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (33): ReLU(inplace=True)\n  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (35): ReLU(inplace=True)\n  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n\n\nIn the paper the authors have replaced the max_pooling layers with average_pooling layers stating that it improves gradient flow and one obtains more appealing results. In the above vgg19 model there are 5 max_pooling layers which we can replace with the following function\n\ndef replace_maxpool_with_avgpool(model):\n    for i, l in enumerate(model.children()):\n        if isinstance(l, nn.MaxPool2d): \n            model[i] = nn.AvgPool2d(kernel_size=2, stride=2)\n\n\nreplace_maxpool_with_avgpool(vgg19)\n\nIn the paper the content image is depicting the Neckarfront in Tubingen, Germany. There are multiple style images: - The Shipwreck of the Minotaur by JMW Turner, 1805. - The Starry Night by Vincent Van Gogh, 1889. - Der Schrei by Edvard Munch, 1893. - Femme nue assise by Pablo Picaso, 1910. - Composition VII by Wassily Kandisky, 1913.\n\ncontent_url = 'https://upload.wikimedia.org/wikipedia/commons/0/00/Tuebingen_Neckarfront.jpg'\n\n\ndef download_image(url):\n    imgb = fc.urlread(url, decode=False)\n    return torchvision.io.decode_image(torch.tensor(list(imgb), dtype=torch.uint8)).to(def_device) / 255\n\n\ncontent_img = download_image(content_url)\nshow_image(content_img, title='Neckarfront');\n\n\n\n\n\n\n\n\n\n\n\nWe can visualize the information at different processing stages in VGG19 by recontructing the above content image from only knowing the network’s responses in a particular layer. For reconstruction we will use white noise image as the starting point.\n\n\nclass TensorModel(nn.Module):\n    def __init__(self, t):\n        super().__init__()\n        self.t = nn.Parameter(t.clone())\n\n    def forward(self, x=0):\n        return self.t\n\n\nmodel = TensorModel(torch.rand_like(content_img))\n[p.shape for p in model.parameters()]\n\n[torch.Size([3, 768, 1024])]\n\n\nIn the paper, the authors picked the layers conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 for the content reconstruction. Here’s a breakdown of what each of these terms means: - conv1_1: The first convolutional layer in the first block. - conv2_1: The first convolutional layer in the second block. - conv3_1: The first convolutional layer in the third block. - conv4_1: The first convolutional layer in the fourth block. - conv5_1: The first convolutional layer in the fifth block.\nVGG19 is structured in a series of convolutional blocks, each containing multiple convolutional layers followed by a max-pooling layer. The naming convention “convX_Y” denotes the Y-th convolutional layer in the X-th block. Here’s the detailed architecture:\nBlock 1:\n    conv1_1: 3x3 convolution, 64 filters\n    conv1_2: 3x3 convolution, 64 filters\n    Max-pooling\n\nBlock 2:\n    conv2_1: 3x3 convolution, 128 filters\n    conv2_2: 3x3 convolution, 128 filters\n    Max-pooling\n\nBlock 3:\n    conv3_1: 3x3 convolution, 256 filters\n    conv3_2: 3x3 convolution, 256 filters\n    conv3_3: 3x3 convolution, 256 filters\n    conv3_4: 3x3 convolution, 256 filters\n    Max-pooling\n\nBlock 4:\n    conv4_1: 3x3 convolution, 512 filters\n    conv4_2: 3x3 convolution, 512 filters\n    conv4_3: 3x3 convolution, 512 filters\n    conv4_4: 3x3 convolution, 512 filters\n    Max-pooling\n\nBlock 5:\n    conv5_1: 3x3 convolution, 512 filters\n    conv5_2: 3x3 convolution, 512 filters\n    conv5_3: 3x3 convolution, 512 filters\n    conv5_4: 3x3 convolution, 512 filters\n    Max-pooling\nSo the layers that we will pick are 0, 5, 10, 19, 28\nThe VGG19 has been trained on imagenet dataset and their mean and standard deviation are [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225] respectively. We will normalize the above content image with the same mean and standard deviation\n\nmean = torch.tensor([0.485, 0.456, 0.406], device=def_device)[:, None, None] # 3, 1, 1\nstd = torch.tensor([0.229, 0.224, 0.225], device=def_device)[:, None, None] # 3, 1, 1\n\n\nmean.shape, std.shape, content_img.shape\n\n(torch.Size([3, 1, 1]), torch.Size([3, 1, 1]), torch.Size([3, 768, 1024]))\n\n\n\ncontent_img.min(), content_img.max(), content_img.mean(), content_img.std()\n\n(tensor(0., device='cuda:0'),\n tensor(1., device='cuda:0'),\n tensor(0.44, device='cuda:0'),\n tensor(0.22, device='cuda:0'))\n\n\n\ndef normalize(img, mean, std):\n    return (img - mean) / std\n\ncontent_norm_img = normalize(content_img, mean, std)\ncontent_norm_img.min(), content_norm_img.max(), content_norm_img.mean(dim=(1,2)), content_norm_img.std(dim=(1,2))\n\n(tensor(-2.12, device='cuda:0'),\n tensor(2.64, device='cuda:0'),\n tensor([-0.35, -0.02,  0.29], device='cuda:0'),\n tensor([0.96, 0.88, 1.10], device='cuda:0'))\n\n\nWe can also normalize using torchvision’s normalize function\n\nfrom torchvision.transforms.functional import normalize\n\n\ncontent_norm_img = normalize(content_img, mean, std)\ncontent_norm_img.min(), content_norm_img.max(), content_norm_img.mean(dim=(1, 2)), content_norm_img.std(dim=(1,2))\n\n(tensor(-2.12, device='cuda:0'),\n tensor(2.64, device='cuda:0'),\n tensor([-0.35, -0.02,  0.29], device='cuda:0'),\n tensor([0.96, 0.88, 1.10], device='cuda:0'))\n\n\nFollowing is the function to get the features from the layers\n\ndef calc_features(x, layers=(0, 5, 10, 19, 28)):\n    x = normalize(x, mean, std)\n    feats = []\n    for i, layer in enumerate(vgg19):\n        x = layer(x)\n        if i in layers:\n            feats.append(x)\n    return feats\n\n\nfeats = calc_features(content_img)\n\n\n[p.shape for p in feats]\n\n[torch.Size([64, 768, 1024]),\n torch.Size([128, 384, 512]),\n torch.Size([256, 192, 256]),\n torch.Size([512, 96, 128]),\n torch.Size([512, 48, 64])]\n\n\nA loss function will be needed to detect how much the starting noisy white image is close to the features\n\nclass ContentLoss(nn.Module):\n    def __init__(self, img, layers=(0, 5, 10, 19, 28)):\n        super().__init__()\n        fc.store_attr()\n        with torch.no_grad():\n            self.feats = calc_features(img, layers=layers)\n\n    def forward(self, x):\n        return sum(F.mse_loss(l1, l2) for l1, l2 in zip(calc_features(x, layers=self.layers), self.feats))\n\nSetting up the infrastructure for training with minai library\n\nclass DummyDataset:\n    def __init__(self, length=1):\n        self.length = length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0, 0\n\n\ndef get_dummy_dls(length):\n    return DataLoaders(DataLoader(DummyDataset(length), 1), DataLoader(DummyDataset(1), 1))\n\n\nclass ImageOptCB(TrainCB):\n    def predict(self, learn): learn.preds = learn.model()\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds)\n\n\nclass ImageLogCB(Callback):\n    order = ProgressCB.order + 1\n    def __init__(self, log_every=10):\n        fc.store_attr()\n        self.i = 0\n        self.imgs = []\n    def after_batch(self, learn):\n        if self.i % self.log_every == 0: self.imgs.append(to_cpu(learn.preds.clip(0, 1)))\n        self.i += 1\n    def after_fit(self, learn):\n        show_images(self.imgs)\n\nWe can try reconstructing the content image with following layers\n\nconv1_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(0,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.048\n0\nTrue\n\n\n0.000\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv2_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(5,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.189\n0\nTrue\n\n\n0.019\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv3_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(10,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.244\n0\nTrue\n\n\n0.037\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv4_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.102\n0\nTrue\n\n\n0.026\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv5_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(28,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.013\n0\nTrue\n\n\n0.002\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\nThe results of the above layers are not that similar to paper’s results. We may required more iterations or changes in hyperparameter to get the same kind of results as in the paper.\n\n\n\n\nThe style representation computes correlation between different features in different layers of the CNN using gram matrix\n\nThe idea here is that we will measure the correlation between features. Given a feature map with f features in an h x w grid, we will flatten out the spatial component and then for every feature we will take the dot product of that row with itself, giving an f x f matrix as the result. Each entry in this matrix qualifies how correlated the relevant pair of features are and how frequently are and how frequently they occur - exactly what we want. In this diagram each feature is represented as colored dot.\n\n\n\nAlt Text\n\n\nBefore creating the infrastructure for calculating the gram matrix and training for style reconstruction, we will download the The Starry Night image by Vincent Van Gogh\n\nstyle_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1513px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\nstyle_img = download_image(style_url)\nshow_image(style_img);\n\n\n\n\n\n\n\n\nNow lets setup the infrastructure\n\ndef calc_gram_matrix(x, layers=(0, 5, 10, 19, 28)):\n    return fc.L(torch.einsum('chw, dhw -&gt; cd', f, f)/ (f.shape[-1] * f.shape[-2]) for f in calc_features(x, layers))\n\n\ngram_matrix = calc_gram_matrix(style_img)\n\n\nclass StyleLoss(nn.Module):\n    def __init__(self, img, layers=(0, 5, 10, 19, 28), layers_weight=(1.0, 0.8, 0.5, 0.3, 0.1)):\n        super().__init__()\n        fc.store_attr()\n        with torch.no_grad():\n            self.gram_matrix = calc_gram_matrix(img, self.layers)\n    def forward(self, x):\n        return sum(w * F.mse_loss(g1, g2) for w, g1, g2 in zip(self.layers_weight, calc_gram_matrix(x, layers=self.layers), self.gram_matrix))\n\nWe can try reconstructing the style image with following layers\n\nconv1_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,), layers_weight=(1.0,))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.002\n0\nTrue\n\n\n0.000\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1 and conv2_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,), layers_weight=(1.0, 0.8))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(240)]\nlearn = Learner(model, get_dummy_dls(1200),  style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.005\n0\nTrue\n\n\n0.000\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1, conv2_1 and conv3_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,), layers_weight=(1.0, 0.8, 0.5,))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.052\n0\nTrue\n\n\n0.001\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1, conv2_1, conv3_1 and conv4_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,19,), layers_weight=(1.0, 0.8, 0.5, 0.3,))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.052\n0\nTrue\n\n\n0.001\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1, conv2_1, conv3_1, conv4_1 and conv5_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,19,28))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.052\n0\nTrue\n\n\n0.001\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(learn.model().clip(0,1));\n\n\n\n\n\n\n\n\n\n\n\nHere we will mix the content image with different style images and see the results\nWe will combine the losses: 1. content_loss and 2. style_loss. In the paper the authors combine both the losses with different ratios $ / $\n\ndef combine_loss(x, alpha=1, beta=1.5):\n    return alpha * content_loss(x) + beta * style_loss(x)\n\n\n\n\nstyle_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1513px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\nstyle_img = download_image(style_url)\nshow_image(style_img);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.179\n0\nTrue\n\n\n0.044\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\nshipwreck_style_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Joseph_Mallord_William_Turner_-_The_Shipwreck_-_Google_Art_Project.jpg/1600px-Joseph_Mallord_William_Turner_-_The_Shipwreck_-_Google_Art_Project.jpg?20110220021219'\nshipwreck_style_image = download_image(shipwreck_style_url)\nshow_image(shipwreck_style_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(shipwreck_style_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.215\n0\nTrue\n\n\n0.070\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\nder_schrei_style_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Edvard_Munch_-_The_Scream_-_Google_Art_Project.jpg/947px-Edvard_Munch_-_The_Scream_-_Google_Art_Project.jpg?20211001183357'\nder_schrei_style_image = download_image(der_schrei_style_url)\nshow_image(der_schrei_style_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(der_schrei_style_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.203\n0\nTrue\n\n\n0.064\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\nfemme_nue_assise_sytle_url = 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8f/Pablo_Picasso%2C_1909-10%2C_Figure_dans_un_Fauteuil_%28Seated_Nude%2C_Femme_nue_assise%29%2C_oil_on_canvas%2C_92.1_x_73_cm%2C_Tate_Modern%2C_London.jpg/919px-Pablo_Picasso%2C_1909-10%2C_Figure_dans_un_Fauteuil_%28Seated_Nude%2C_Femme_nue_assise%29%2C_oil_on_canvas%2C_92.1_x_73_cm%2C_Tate_Modern%2C_London.jpg?20150127060348'\nfemme_nue_assise_sytle_image = download_image(femme_nue_assise_sytle_url)\nshow_image(femme_nue_assise_sytle_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(femme_nue_assise_sytle_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.227\n0\nTrue\n\n\n0.080\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\ncomposition_7_style_url = 'https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg'\ncomposition_7_sytle_image = download_image(composition_7_style_url)\nshow_image(composition_7_sytle_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(composition_7_sytle_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.324\n0\nTrue\n\n\n0.143\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));"
  },
  {
    "objectID": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html#content-reconstruction",
    "href": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html#content-reconstruction",
    "title": "Paper Implementation: A Neural Algorithm of Artistic Style",
    "section": "",
    "text": "We can visualize the information at different processing stages in VGG19 by recontructing the above content image from only knowing the network’s responses in a particular layer. For reconstruction we will use white noise image as the starting point.\n\n\nclass TensorModel(nn.Module):\n    def __init__(self, t):\n        super().__init__()\n        self.t = nn.Parameter(t.clone())\n\n    def forward(self, x=0):\n        return self.t\n\n\nmodel = TensorModel(torch.rand_like(content_img))\n[p.shape for p in model.parameters()]\n\n[torch.Size([3, 768, 1024])]\n\n\nIn the paper, the authors picked the layers conv1_1, conv2_1, conv3_1, conv4_1 and conv5_1 for the content reconstruction. Here’s a breakdown of what each of these terms means: - conv1_1: The first convolutional layer in the first block. - conv2_1: The first convolutional layer in the second block. - conv3_1: The first convolutional layer in the third block. - conv4_1: The first convolutional layer in the fourth block. - conv5_1: The first convolutional layer in the fifth block.\nVGG19 is structured in a series of convolutional blocks, each containing multiple convolutional layers followed by a max-pooling layer. The naming convention “convX_Y” denotes the Y-th convolutional layer in the X-th block. Here’s the detailed architecture:\nBlock 1:\n    conv1_1: 3x3 convolution, 64 filters\n    conv1_2: 3x3 convolution, 64 filters\n    Max-pooling\n\nBlock 2:\n    conv2_1: 3x3 convolution, 128 filters\n    conv2_2: 3x3 convolution, 128 filters\n    Max-pooling\n\nBlock 3:\n    conv3_1: 3x3 convolution, 256 filters\n    conv3_2: 3x3 convolution, 256 filters\n    conv3_3: 3x3 convolution, 256 filters\n    conv3_4: 3x3 convolution, 256 filters\n    Max-pooling\n\nBlock 4:\n    conv4_1: 3x3 convolution, 512 filters\n    conv4_2: 3x3 convolution, 512 filters\n    conv4_3: 3x3 convolution, 512 filters\n    conv4_4: 3x3 convolution, 512 filters\n    Max-pooling\n\nBlock 5:\n    conv5_1: 3x3 convolution, 512 filters\n    conv5_2: 3x3 convolution, 512 filters\n    conv5_3: 3x3 convolution, 512 filters\n    conv5_4: 3x3 convolution, 512 filters\n    Max-pooling\nSo the layers that we will pick are 0, 5, 10, 19, 28\nThe VGG19 has been trained on imagenet dataset and their mean and standard deviation are [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225] respectively. We will normalize the above content image with the same mean and standard deviation\n\nmean = torch.tensor([0.485, 0.456, 0.406], device=def_device)[:, None, None] # 3, 1, 1\nstd = torch.tensor([0.229, 0.224, 0.225], device=def_device)[:, None, None] # 3, 1, 1\n\n\nmean.shape, std.shape, content_img.shape\n\n(torch.Size([3, 1, 1]), torch.Size([3, 1, 1]), torch.Size([3, 768, 1024]))\n\n\n\ncontent_img.min(), content_img.max(), content_img.mean(), content_img.std()\n\n(tensor(0., device='cuda:0'),\n tensor(1., device='cuda:0'),\n tensor(0.44, device='cuda:0'),\n tensor(0.22, device='cuda:0'))\n\n\n\ndef normalize(img, mean, std):\n    return (img - mean) / std\n\ncontent_norm_img = normalize(content_img, mean, std)\ncontent_norm_img.min(), content_norm_img.max(), content_norm_img.mean(dim=(1,2)), content_norm_img.std(dim=(1,2))\n\n(tensor(-2.12, device='cuda:0'),\n tensor(2.64, device='cuda:0'),\n tensor([-0.35, -0.02,  0.29], device='cuda:0'),\n tensor([0.96, 0.88, 1.10], device='cuda:0'))\n\n\nWe can also normalize using torchvision’s normalize function\n\nfrom torchvision.transforms.functional import normalize\n\n\ncontent_norm_img = normalize(content_img, mean, std)\ncontent_norm_img.min(), content_norm_img.max(), content_norm_img.mean(dim=(1, 2)), content_norm_img.std(dim=(1,2))\n\n(tensor(-2.12, device='cuda:0'),\n tensor(2.64, device='cuda:0'),\n tensor([-0.35, -0.02,  0.29], device='cuda:0'),\n tensor([0.96, 0.88, 1.10], device='cuda:0'))\n\n\nFollowing is the function to get the features from the layers\n\ndef calc_features(x, layers=(0, 5, 10, 19, 28)):\n    x = normalize(x, mean, std)\n    feats = []\n    for i, layer in enumerate(vgg19):\n        x = layer(x)\n        if i in layers:\n            feats.append(x)\n    return feats\n\n\nfeats = calc_features(content_img)\n\n\n[p.shape for p in feats]\n\n[torch.Size([64, 768, 1024]),\n torch.Size([128, 384, 512]),\n torch.Size([256, 192, 256]),\n torch.Size([512, 96, 128]),\n torch.Size([512, 48, 64])]\n\n\nA loss function will be needed to detect how much the starting noisy white image is close to the features\n\nclass ContentLoss(nn.Module):\n    def __init__(self, img, layers=(0, 5, 10, 19, 28)):\n        super().__init__()\n        fc.store_attr()\n        with torch.no_grad():\n            self.feats = calc_features(img, layers=layers)\n\n    def forward(self, x):\n        return sum(F.mse_loss(l1, l2) for l1, l2 in zip(calc_features(x, layers=self.layers), self.feats))\n\nSetting up the infrastructure for training with minai library\n\nclass DummyDataset:\n    def __init__(self, length=1):\n        self.length = length\n    def __len__(self): return self.length\n    def __getitem__(self, idx): return 0, 0\n\n\ndef get_dummy_dls(length):\n    return DataLoaders(DataLoader(DummyDataset(length), 1), DataLoader(DummyDataset(1), 1))\n\n\nclass ImageOptCB(TrainCB):\n    def predict(self, learn): learn.preds = learn.model()\n    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds)\n\n\nclass ImageLogCB(Callback):\n    order = ProgressCB.order + 1\n    def __init__(self, log_every=10):\n        fc.store_attr()\n        self.i = 0\n        self.imgs = []\n    def after_batch(self, learn):\n        if self.i % self.log_every == 0: self.imgs.append(to_cpu(learn.preds.clip(0, 1)))\n        self.i += 1\n    def after_fit(self, learn):\n        show_images(self.imgs)\n\nWe can try reconstructing the content image with following layers\n\nconv1_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(0,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.048\n0\nTrue\n\n\n0.000\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv2_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(5,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.189\n0\nTrue\n\n\n0.019\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv3_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(10,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.244\n0\nTrue\n\n\n0.037\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv4_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.102\n0\nTrue\n\n\n0.026\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\nconv5_1\n\n\ncontent_loss = ContentLoss(content_img, layers=(28,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(30)]\nlearn = Learner(model, get_dummy_dls(150), content_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.013\n0\nTrue\n\n\n0.002\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\nThe results of the above layers are not that similar to paper’s results. We may required more iterations or changes in hyperparameter to get the same kind of results as in the paper."
  },
  {
    "objectID": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html#style-reconstruction",
    "href": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html#style-reconstruction",
    "title": "Paper Implementation: A Neural Algorithm of Artistic Style",
    "section": "",
    "text": "The style representation computes correlation between different features in different layers of the CNN using gram matrix\n\nThe idea here is that we will measure the correlation between features. Given a feature map with f features in an h x w grid, we will flatten out the spatial component and then for every feature we will take the dot product of that row with itself, giving an f x f matrix as the result. Each entry in this matrix qualifies how correlated the relevant pair of features are and how frequently are and how frequently they occur - exactly what we want. In this diagram each feature is represented as colored dot.\n\n\n\nAlt Text\n\n\nBefore creating the infrastructure for calculating the gram matrix and training for style reconstruction, we will download the The Starry Night image by Vincent Van Gogh\n\nstyle_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1513px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\nstyle_img = download_image(style_url)\nshow_image(style_img);\n\n\n\n\n\n\n\n\nNow lets setup the infrastructure\n\ndef calc_gram_matrix(x, layers=(0, 5, 10, 19, 28)):\n    return fc.L(torch.einsum('chw, dhw -&gt; cd', f, f)/ (f.shape[-1] * f.shape[-2]) for f in calc_features(x, layers))\n\n\ngram_matrix = calc_gram_matrix(style_img)\n\n\nclass StyleLoss(nn.Module):\n    def __init__(self, img, layers=(0, 5, 10, 19, 28), layers_weight=(1.0, 0.8, 0.5, 0.3, 0.1)):\n        super().__init__()\n        fc.store_attr()\n        with torch.no_grad():\n            self.gram_matrix = calc_gram_matrix(img, self.layers)\n    def forward(self, x):\n        return sum(w * F.mse_loss(g1, g2) for w, g1, g2 in zip(self.layers_weight, calc_gram_matrix(x, layers=self.layers), self.gram_matrix))\n\nWe can try reconstructing the style image with following layers\n\nconv1_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,), layers_weight=(1.0,))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.002\n0\nTrue\n\n\n0.000\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1 and conv2_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,), layers_weight=(1.0, 0.8))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(240)]\nlearn = Learner(model, get_dummy_dls(1200),  style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.005\n0\nTrue\n\n\n0.000\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1, conv2_1 and conv3_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,), layers_weight=(1.0, 0.8, 0.5,))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.052\n0\nTrue\n\n\n0.001\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1, conv2_1, conv3_1 and conv4_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,19,), layers_weight=(1.0, 0.8, 0.5, 0.3,))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.052\n0\nTrue\n\n\n0.001\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconv1_1, conv2_1, conv3_1, conv4_1 and conv5_1\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,19,28))\nmodel = TensorModel(torch.rand_like(style_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), style_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.052\n0\nTrue\n\n\n0.001\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(learn.model().clip(0,1));"
  },
  {
    "objectID": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html#content-style-representation",
    "href": "posts/Paper_Implementation_A_Neural_Algorithm_of_Artistic_Style/index.html#content-style-representation",
    "title": "Paper Implementation: A Neural Algorithm of Artistic Style",
    "section": "",
    "text": "Here we will mix the content image with different style images and see the results\nWe will combine the losses: 1. content_loss and 2. style_loss. In the paper the authors combine both the losses with different ratios $ / $\n\ndef combine_loss(x, alpha=1, beta=1.5):\n    return alpha * content_loss(x) + beta * style_loss(x)\n\n\n\n\nstyle_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1513px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\nstyle_img = download_image(style_url)\nshow_image(style_img);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(style_img, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.179\n0\nTrue\n\n\n0.044\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\nshipwreck_style_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Joseph_Mallord_William_Turner_-_The_Shipwreck_-_Google_Art_Project.jpg/1600px-Joseph_Mallord_William_Turner_-_The_Shipwreck_-_Google_Art_Project.jpg?20110220021219'\nshipwreck_style_image = download_image(shipwreck_style_url)\nshow_image(shipwreck_style_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(shipwreck_style_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.215\n0\nTrue\n\n\n0.070\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\nder_schrei_style_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/86/Edvard_Munch_-_The_Scream_-_Google_Art_Project.jpg/947px-Edvard_Munch_-_The_Scream_-_Google_Art_Project.jpg?20211001183357'\nder_schrei_style_image = download_image(der_schrei_style_url)\nshow_image(der_schrei_style_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(der_schrei_style_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\n\n\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.203\n0\nTrue\n\n\n0.064\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\nfemme_nue_assise_sytle_url = 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8f/Pablo_Picasso%2C_1909-10%2C_Figure_dans_un_Fauteuil_%28Seated_Nude%2C_Femme_nue_assise%29%2C_oil_on_canvas%2C_92.1_x_73_cm%2C_Tate_Modern%2C_London.jpg/919px-Pablo_Picasso%2C_1909-10%2C_Figure_dans_un_Fauteuil_%28Seated_Nude%2C_Femme_nue_assise%29%2C_oil_on_canvas%2C_92.1_x_73_cm%2C_Tate_Modern%2C_London.jpg?20150127060348'\nfemme_nue_assise_sytle_image = download_image(femme_nue_assise_sytle_url)\nshow_image(femme_nue_assise_sytle_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(femme_nue_assise_sytle_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.227\n0\nTrue\n\n\n0.080\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));\n\n\n\n\n\n\n\n\n\n\n\n\ncomposition_7_style_url = 'https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg'\ncomposition_7_sytle_image = download_image(composition_7_style_url)\nshow_image(composition_7_sytle_image);\n\n\n\n\n\n\n\n\n\nstyle_loss = StyleLoss(composition_7_sytle_image, layers=(0,5,10,19,28))\ncontent_loss = ContentLoss(content_img, layers=(19,))\nmodel = TensorModel(torch.rand_like(content_img))\ncbs = [DeviceCB(), MetricsCB(), ProgressCB(plot=True), ImageOptCB(), ImageLogCB(60)]\nlearn = Learner(model, get_dummy_dls(300), combine_loss, cbs=cbs, opt_func=optim.Adam)\nlearn.fit(1)\n\n\n\n\n\n\n\n\nloss\nepoch\ntrain\n\n\n\n\n0.324\n0\nTrue\n\n\n0.143\n0\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_image(to_cpu(learn.model().clip(0, 1)));"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anubhav’s blog",
    "section": "",
    "text": "Paper Implementation: A Neural Algorithm of Artistic Style\n\n\n\n\n\nMy implementation of the paper A Neural Algorithm of Artistic Style\n\n\n\n\n\nJun 22, 2024\n\n\nAnubhav Maity\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Local Sensitive Hashing: Loop-Free Implementation\n\n\n\n\n\nMy implementation of Local Sensitive Hashing with zero loop - only tensor operations\n\n\n\n\n\nSep 11, 2023\n\n\nAnubhav Maity\n\n\n\n\n\n\n\n\n\n\n\n\nMovie Genre Predictions with Hugging Face Transformers\n\n\n\n\n\nMy attempt on the Movie Genre Predictions competition\n\n\n\n\n\nSep 6, 2023\n\n\nAnubhav Maity\n\n\n\n\n\n\nNo matching items"
  }
]